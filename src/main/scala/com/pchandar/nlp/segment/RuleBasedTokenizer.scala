package com.pchandar.nlp.segment

import java.io.StringReader

import cc.factorie.app.nlp.{Document, Token}
import com.typesafe.config.Config

import scalaz.EphemeralStream

/**
  *
  * Split a String into a sequence of Tokens.
  * This is a version adopted from factorie, therefore aims to adhere to
  * tokenization rules used in Ontonotes and PennTreebank.
  *
  * Punctuation that ends a sentence should be placed alone in its own Token,
  * hence this segmentation implicitly defines sentence segmentation also.
  *
  * Should be fast since it uses the DFA automatically generated by JFlex using the
  * definition in PTBLexer.flex
  *
  * NOTE: This tokenizer can also normalize, but it is currently not implemented.
  *
  */

class RuleBasedTokenizer(config: Option[Config] = None)  {
  // Load Tokenizer Config
  val default_opts = "invertible=true,ptb3Escaping=true,tokenizeNLs=true,normalizeFractions=false"
  val tokenizerOpts = config match {
    case Some(s) =>
      if (s.hasPath("tokenizerOpts")) s.getString("tokenizerOpts")
      else default_opts
    case _ => default_opts
  }


  val lf = new LexerFactory


  def process(document: Document): Document = {
    for (section <- document.sections.toStream) {
      /* Add this newline to avoid JFlex issue where we can't match EOF with lookahead */
      val reader = new StringReader(section.string)
      val lexer = {
        val _lexer = new PTBLexer(reader, lf, tokenizerOpts)
        def iterate: EphemeralStream[(String, Int, Int)] = {
          val r = _lexer.next().asInstanceOf[(String, Int, Int)]
          if (r == null) EphemeralStream[(String, Int, Int)]()
          else
            r ##:: iterate
        }
        iterate
      }

      val tot = lexer.foldLeft(0)(x => currentToken => {
        val tok = new Token(section, currentToken._2, currentToken._2 + currentToken._3)
        x+ 1})

    }
    if (!document.annotators.contains(classOf[Token]))
      document.annotators(classOf[Token]) = this.getClass
    document
  }


  /** Convenience function to run the tokenizer on an arbitrary String.  The implementation builds a Document internally, then maps to token strings. */
  def apply(s: String): Seq[String] = process(new Document(s)).tokens.toSeq.map(_.string)

  def getTokenFromString(s: String): Seq[Token] = process(new Document(s)).tokens.toSeq
}

